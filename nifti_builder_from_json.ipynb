{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from library_dicom.dicom_processor.model.Series import Series \n",
    "from library_dicom.dicom_processor.model.SeriesPT import SeriesPT\n",
    "from library_dicom.dicom_processor.model.SeriesCT import SeriesCT\n",
    "from library_dicom.dicom_processor.model.csv_reader.MaskBuilder import MaskBuilder\n",
    "\n",
    "from library_dicom.dicom_processor.tools.folders import *\n",
    "from library_dicom.dicom_processor.tools.create_mip import *\n",
    "from library_dicom.dicom_processor.tools.threshold_mask import *\n",
    "\n",
    "import numpy as np\n",
    "import SimpleITK as sitk\n",
    "import csv"
   ]
  },
  {
   "source": [
    "#### - Get dataset from  PARSE_FILTERED_DICOM_DATASET script"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#LIST_PATH from json\n",
    "json_path = '/media/deeplearning/LACIE SHARE/relevance_dataset.json'\n",
    "dataset = []\n",
    "with open(json_path) as json_file : \n",
    "    reader = json.load(json_file)\n",
    "    for info in reader :\n",
    "        dataset.append(info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_study = len(dataset)\n",
    "print(\"number of study : \", number_of_study)"
   ]
  },
  {
   "source": [
    "#### - Add CSV file for every study on the dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#add csv_file from csv \n",
    "\n",
    "csv_path= '/home/deeplearning/AHL/AHL_CSV_DATA/pet0.csv'\n",
    "csv_file = []\n",
    "with open(csv_path, 'r') as csv_file :\n",
    "    reader = csv.reader(csv_file, delimiter = ';') #liste pour chaque ligne \n",
    "    csv_file = []\n",
    "    for row in reader :\n",
    "        csv_file.append(row)\n",
    "        \n",
    "del csv_file[0] #enlever premi√®re ligne"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#add csv_file from a list \n",
    "\n",
    "csv_path = '/media/deeplearning/LACIE SHARE/RELEVANCE Fichier CSV'\n",
    "liste_csv_file = os.listdir(csv_path)\n",
    "liste_csv = []\n",
    "for csv_file in liste_csv_file : \n",
    "    if \"csv\" in csv_file : \n",
    "        liste_csv.append(os.path.join(csv_path, csv_file))\n",
    "\n",
    "#print(liste_csv)\n",
    "for data in dataset : \n",
    "    #nom = '_'.join(data[-1].split('_')[0:2])\n",
    "    #print(nom)\n",
    "    patient_id = data[-1]\n",
    "    for csv_f in liste_csv : \n",
    "        if patient_id in csv_f: \n",
    "            data.append(csv_f)"
   ]
  },
  {
   "source": [
    "#### - Verify if every study has a csv file, \n",
    "####            If not, write a json , remove study from dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "no_csv = []\n",
    "for data in dataset : \n",
    "    if len(data) == 8 : \n",
    "        no_csv.append(data)\n",
    "print(\"Nbr de study sans csv :\", len(no_csv))\n",
    "write_json_file('/media/deeplearning/LACIE SHARE/RELEVANCE Fichier CSV', 'study_no_csv', no_csv)\n",
    "try : \n",
    "    for err in no_csv : \n",
    "        dataset.remove(err)\n",
    "except Exception as err : \n",
    "    print(err) "
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "#### - If a study has more than 1 CSV, keep CSV with higher volume"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from library_dicom.dicom_processor.tools.csv_volume import *\n",
    "for data in dataset : \n",
    "    liste_csv = []\n",
    "    if len(data) != 9 : #more than 1 csv \n",
    "        for item in data : \n",
    "            if 'csv' in item : \n",
    "                liste_csv.append(item)\n",
    "        #print(liste_csv)\n",
    "        wrong_csv = keep_csv_higher_volume(liste_csv)\n",
    "        #print(\"remove : \", remove)\n",
    "        try : \n",
    "            for r in wrong_csv : \n",
    "                data.remove(r)\n",
    "        except Exception as err : \n",
    "            print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check \n",
    "for data in dataset : \n",
    "    if len(data) != 9 : \n",
    "        print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "#### -  Rewrite path and save complete dataset as json file "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#rewrite path \n",
    "\n",
    "for data in dataset : \n",
    "    new_1 = data[0].replace('/media/oncopole/DD 2To', '/media/deeplearning/Elements')\n",
    "    new_2 = data[2].replace('/media/oncopole/DD 2To', '/media/deeplearning/Elements')\n",
    "    data[0] = new_1\n",
    "    data[2] = new_2"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Nombre de study : ', len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_json_file('/media/deeplearning/LACIE SHARE', 'RELEVANCE_validated_dataset', dataset)"
   ]
  },
  {
   "source": [
    "#### - Remove remove_bi_file if it exists in every serie "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from library_dicom.dicom_processor.tools.folders import *\n",
    "directory_validated_dicom = ''\n",
    "paths = get_series_path(directory_validated_dicom)\n",
    "\n",
    "for path in paths : \n",
    "    remove_bi_file(path)"
   ]
  },
  {
   "source": [
    "### Different scripts : "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### - Check if there is Unconstance Spacing error in every study of the dataset "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "unconstant_spacing = []\n",
    "serie_error = []\n",
    "for serie in dataset : \n",
    "    print(dataset.index(serie))\n",
    "    try : \n",
    "        if serie[1] =='PT' : \n",
    "            serie_pt_objet = SeriesPT(serie[0])\n",
    "            nifti_array = serie_pt_objet.get_numpy_array()\n",
    "            if serie_pt_objet.get_z_spacing =='Unconstant Spacing' :\n",
    "                print(\"Unconstant Spacing PET\")\n",
    "                unconstant_spacing.append(serie)\n",
    "            \n",
    "            serie_ct_objet = SeriesCT(serie[2])\n",
    "            nifti_array_ct = serie_ct_objet.get_numpy_array()\n",
    "            if serie_ct_objet.get_z_spacing =='Unconstant Spacing' :\n",
    "                print(\"Unconstant Spacing CT\")\n",
    "                unconstant_spacing.append(serie)\n",
    "\n",
    "        else : \n",
    "            serie_pt_objet = SeriesPT(serie[2])\n",
    "            nifti_array = serie_pt_objet.get_numpy_array()\n",
    "            if serie_pt_objet.get_z_spacing =='Unconstant Spacing' :\n",
    "                print(\"Unconstant Spacing PT\")\n",
    "                unconstant_spacing.append(serie)\n",
    "            serie_ct_objet = SeriesCT(serie[0])\n",
    "            nifti_array_ct = serie_ct_objet.get_numpy_array()\n",
    "            if serie_ct_objet.get_z_spacing =='Unconstant Spacing' :\n",
    "                print(\"Unconstant Spacing CT\")\n",
    "                unconstant_spacing.append(serie)\n",
    "\n",
    "    except Exception as err : \n",
    "        print(serie)\n",
    "        print(err)\n",
    "        serie_error.append(serie)\n"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "#### - Generate PET, CT, MASK NIFTI with checking suv_max, mean and sd"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#Generate PET, CT, MASK NIFTI with checking suv_max, mean and sd\n",
    "\n",
    "nifti_directory = '/media/deeplearning/LACIE SHARE/RELEVANCE/NIFTI'\n",
    "mip_directory = '/media/deeplearning/LACIE SHARE/RELEVANCE/MIP'\n",
    "\n",
    "#save serie_path with false mask \n",
    "serie_false_mask = []\n",
    "#save result about serie with false mask \n",
    "results_false_mask = []\n",
    "#save path of MIP to generate PDF \n",
    "path_mip = []\n",
    "\n",
    "#save error serie \n",
    "error_dataset = []\n",
    "\n",
    "for serie in dataset: \n",
    "    try : \n",
    "        print(dataset.index(serie))\n",
    "        subliste = []\n",
    "        if serie[1] == 'PT' : \n",
    "            \n",
    "            serie_pt_objet = SeriesPT(serie[0])\n",
    "            \n",
    "            serie_pt_objet.get_instances_ordered()\n",
    "            nifti_array = serie_pt_objet.get_numpy_array()\n",
    "            study_uid = serie_pt_objet.get_series_details()['study']['StudyInstanceUID']\n",
    "            size_matrix = serie_pt_objet.get_size_matrix()\n",
    "            mask_objet = MaskBuilder(serie[-1], size_matrix)\n",
    "            mask_4D = mask_objet.mask_array\n",
    "\n",
    "            number_roi = mask_4D.shape[3]\n",
    "            threshold = mask_objet.details_rois['SUVlo']\n",
    "\n",
    "            if mask_objet.is_correct_suv(nifti_array) == True : \n",
    "\n",
    "                print(\"MASK CORRECT\")\n",
    "                #generation nifti PT\n",
    "                filename_pt = study_uid+'_'+'nifti_'+'PT'+'.nii'\n",
    "                #serie_pt_objet.export_nifti(os.path.join(nifti_directory, filename_pt))\n",
    "                print(\"EXPORT NIFTI PT\")\n",
    "\n",
    "                #generation nifti CT\n",
    "                serie_ct_objet = SeriesCT(serie[2])\n",
    "                serie_ct_objet.get_instances_ordered()\n",
    "                serie_ct_objet.get_numpy_array()\n",
    "                filename_ct = study_uid+'_'+'nifti_'+'CT'+'.nii'\n",
    "                #serie_ct_objet.export_nifti(os.path.join(nifti_directory, filename_ct))\n",
    "                print(\"EXPORT NIFTI CT\")\n",
    "\n",
    "                #generation nifti mask\n",
    "                filename_mask = study_uid+'_'+'nifti_'+'mask'+'.nii'\n",
    "                serie_pt_objet.export_nifti(os.path.join(nifti_directory, filename_mask), mask_4D)\n",
    "                print(\"EXPORT NIFTI MASK\")\n",
    "\n",
    "\n",
    "            else : \n",
    "                results = []\n",
    "                print(\"FALSE MASK\")\n",
    "                serie_false_mask.append(serie)\n",
    "\n",
    "                print(mask_objet.calcul_suv(nifti_array))\n",
    "                results.append(mask_objet.calcul_suv(nifti_array))\n",
    "\n",
    "                print(mask_objet.ecart_suv_max(nifti_array))\n",
    "                results.append(mask_objet.ecart_suv_max(nifti_array))\n",
    "\n",
    "                print(mask_objet.ecart_suv_mean(nifti_array))\n",
    "                results.append(mask_objet.ecart_suv_mean(nifti_array))\n",
    "\n",
    "                print(mask_objet.ecart_SD(nifti_array))\n",
    "                results.append(mask_objet.ecart_SD(nifti_array))\n",
    "\n",
    "                results_false_mask.append(results)\n",
    "\n",
    "                #threshold mask 41% \n",
    "                mask_4D = threshold_matrix(mask_4D, nifti_array, 0.41)\n",
    "\n",
    "                #create mip for false mask and check \n",
    "                #pet\n",
    "                angle_filename = mip_projection(nifti_array, 0, mip_directory, study_uid, 'pet', cmap='Greys', vmin=0, vmax=7) \n",
    "                subliste.append(angle_filename)\n",
    "                print('MIP PET')\n",
    "               # mask\n",
    "                angle_filename_mask = mip_projection_4D(mask_4D, 0, mip_directory, study_uid, number_roi, cmap='Greys')\n",
    "                subliste.append(angle_filename_mask)\n",
    "                print('MIP MASK')\n",
    "                path_mip.append(subliste)\n",
    "\n",
    "\n",
    "        else : \n",
    "            serie_pt_objet = SeriesPT(serie[2])\n",
    "            serie_pt_objet.get_instances_ordered()\n",
    "            nifti_array = serie_pt_objet.get_numpy_array()\n",
    "            study_uid = serie_pt_objet.get_series_details()['study']['StudyInstanceUID']\n",
    "            size_matrix = serie_pt_objet.get_size_matrix()\n",
    "            mask_objet = MaskBuilder(serie[-1], size_matrix)\n",
    "            mask_4D = mask_objet.mask_array\n",
    "\n",
    "            number_roi = mask_4D.shape[3]\n",
    "            threshold = mask_objet.details_rois['SUVlo']\n",
    "\n",
    "            if mask_objet.is_correct_suv(nifti_array) == True :  \n",
    "                print(\"MASK CORRECT\")\n",
    "                #generation nifti PT\n",
    "                filename_pt = study_uid+'_'+'nifti_'+'PT'+'.nii'\n",
    "                #serie_pt_objet.export_nifti(os.path.join(nifti_directory, filename_pt))\n",
    "                print(\"EXPORT NIFTI PT\")\n",
    "\n",
    "                #generation nifti CT\n",
    "                serie_ct_objet = SeriesCT(serie[0])\n",
    "                serie_ct_objet.get_instances_ordered()\n",
    "                serie_ct_objet.get_numpy_array()\n",
    "                filename_ct = study_uid+'_'+'nifti_'+'CT'+'.nii'\n",
    "                #serie_ct_objet.export_nifti(os.path.join(nifti_directory, filename_ct))\n",
    "                print(\"EXPORT NIFTI CT\")\n",
    "\n",
    "                #generation nifti mask\n",
    "                filename_mask = study_uid+'_'+'nifti_'+'mask'+'.nii'\n",
    "                serie_pt_objet.export_nifti(os.path.join(nifti_directory, filename_mask), mask_4D)\n",
    "                print(\"EXPORT NIFTI MASK\")\n",
    "\n",
    "\n",
    "\n",
    "            else : \n",
    "                results = []\n",
    "                print(\"FALSE MASK\")\n",
    "                serie_false_mask.append(serie)\n",
    "\n",
    "                print(mask_objet.calcul_suv(nifti_array))\n",
    "                results.append(mask_objet.calcul_suv(nifti_array))\n",
    "\n",
    "                print(mask_objet.ecart_suv_max(nifti_array))\n",
    "                results.append(mask_objet.ecart_suv_max(nifti_array))\n",
    "\n",
    "                print(mask_objet.ecart_suv_mean(nifti_array))\n",
    "                results.append(mask_objet.ecart_suv_mean(nifti_array))\n",
    "\n",
    "                print(mask_objet.ecart_SD(nifti_array))\n",
    "                results.append(mask_objet.ecart_SD(nifti_array))\n",
    "\n",
    "                results_false_mask.append(results)\n",
    "\n",
    "                #threshold mask 41% \n",
    "                mask_4D = threshold_matrix(mask_4D, nifti_array, 0.41)\n",
    "\n",
    "                #create mip for false mask and check \n",
    "                #pet\n",
    "                angle_filename = mip_projection(nifti_array, 0, mip_directory, study_uid, 'pet', cmap='Greys', vmin=0, vmax=7) \n",
    "                subliste.append(angle_filename)\n",
    "                print(\"MIP PET\")\n",
    "                #mask\n",
    "                angle_filename_mask = mip_projection_4D(mask_4D, 0, mip_directory, study_uid, number_roi, cmap='Greys')\n",
    "                subliste.append(angle_filename_mask)\n",
    "                print('MIP MASK ')\n",
    "                path_mip.append(subliste)\n",
    "               \n",
    "\n",
    "    except Exception as err : \n",
    "        print(serie)\n",
    "        print(err)\n",
    "        error_dataset.append(serie)"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "print(\"number of error : \", len(error_dataset))\n",
    "print(\"number of false mask : \" , len(serie_false_mask))"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "#Save different json \n",
    "directory_nifti = '/media/deeplearning/LACIE SHARE/RELEVANCE'\n",
    "write_json_file(directory_nifti, 'false_mask_serie', serie_false_mask)\n",
    "write_json_file(directory_nifti, 'false_mask_results', results_false_mask)\n",
    "#write_json_file(directory_nifti, 'error', error_dataset)\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "#### - Generate PET, CT, MASK NIFTI without checking"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = ''\n",
    "dataset = []\n",
    "with open(json_path) as json_file : \n",
    "    reader = json.load(json_file)\n",
    "    for info in reader :\n",
    "        dataset.append(info)"
   ]
  },
  {
   "source": [
    "nifti_directory = '/media/deeplearning/LACIE SHARE/RELEVANCE/NIFTI'\n",
    "\n",
    "error_dataset = []\n",
    "\n",
    "for serie in dataset: \n",
    "    try : \n",
    "        print(dataset.index(serie))\n",
    "\n",
    "        if serie[1] == 'PT' : \n",
    "            serie_pt_objet = SeriesPT(serie[0])\n",
    "            serie_pt_objet.get_instances_ordered()\n",
    "            nifti_array = serie_pt_objet.get_numpy_array()\n",
    "            study_uid = serie_pt_objet.get_series_details()['study']['StudyInstanceUID']\n",
    "            size_matrix = serie_pt_objet.get_size_matrix()\n",
    "            mask_objet = MaskBuilder(serie[-1], size_matrix)\n",
    "            mask_4D = mask_objet.mask_array\n",
    "\n",
    "            number_roi = mask_4D.shape[3]\n",
    "\n",
    "            print(\"MASK CORRECT\")\n",
    "                    #generation nifti PT\n",
    "            filename_pt = study_uid+'_'+'nifti_'+'PT'+'.nii'\n",
    "            #serie_pt_objet.export_nifti(os.path.join(nifti_directory, filename_pt))\n",
    "            print(\"EXPORT NIFTI PT\")\n",
    "\n",
    "                    #generation nifti CT\n",
    "            serie_ct_objet = SeriesCT(serie[2])\n",
    "            serie_ct_objet.get_instances_ordered()\n",
    "            serie_ct_objet.get_numpy_array()\n",
    "            filename_ct = study_uid+'_'+'nifti_'+'CT'+'.nii'\n",
    "            #serie_ct_objet.export_nifti(os.path.join(nifti_directory, filename_ct))\n",
    "            print(\"EXPORT NIFTI CT\")\n",
    "\n",
    "                    #generation nifti mask\n",
    "            filename_mask = study_uid+'_'+'nifti_'+'mask'+'.nii'\n",
    "            serie_pt_objet.export_nifti(os.path.join(nifti_directory, filename_mask), mask_4D)\n",
    "            print(\"EXPORT NIFTI MASK\")\n",
    "          \n",
    "          \n",
    "        else : \n",
    "            serie_pt_objet = SeriesPT(serie[2])\n",
    "            serie_pt_objet.get_instances_ordered()\n",
    "            nifti_array = serie_pt_objet.get_numpy_array()\n",
    "            study_uid = serie_pt_objet.get_series_details()['study']['StudyInstanceUID']\n",
    "            size_matrix = serie_pt_objet.get_size_matrix()\n",
    "            mask_objet = MaskBuilder(serie[-1], size_matrix)\n",
    "            mask_4D = mask_objet.mask_array\n",
    "            number_roi = mask_4D.shape[3]\n",
    "\n",
    "            print(\"MASK CORRECT\")\n",
    "                    #generation nifti PT\n",
    "            filename_pt = study_uid+'_'+'nifti_'+'PT'+'.nii'\n",
    "            #serie_pt_objet.export_nifti(os.path.join(nifti_directory, filename_pt))\n",
    "            print(\"EXPORT NIFTI PT\")\n",
    "\n",
    "                     #generation nifti CT\n",
    "            serie_ct_objet = SeriesCT(serie[0])\n",
    "            serie_ct_objet.get_instances_ordered()\n",
    "            serie_ct_objet.get_numpy_array()\n",
    "            filename_ct = study_uid+'_'+'nifti_'+'CT'+'.nii'\n",
    "            #serie_ct_objet.export_nifti(os.path.join(nifti_directory, filename_ct))\n",
    "            print(\"EXPORT NIFTI CT\")\n",
    "\n",
    "                    #generation nifti mask\n",
    "            filename_mask = study_uid+'_'+'nifti_'+'mask'+'.nii'\n",
    "            serie_pt_objet.export_nifti(os.path.join(nifti_directory, filename_mask), mask_4D)\n",
    "            print(\"EXPORT NIFTI MASK\")\n",
    "           \n",
    "    except Exception as err : \n",
    "        print(err)\n",
    "        print(serie)\n",
    "        error_dataset.append(serie)"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of error : \", len(error_dataset))"
   ]
  },
  {
   "source": [
    "#### - Generate PET & CT NIFTI only"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "nifti_directory = '/media/deeplearning/LACIE SHARE/Olivier_Morel_sarcome'\n",
    "\n",
    "error = []\n",
    "for serie in dataset : \n",
    "    print(dataset.index(serie))\n",
    "    try : \n",
    "        if serie[1] == 'PT' : \n",
    "                serie_pt_objet = SeriesPT(serie[0]) \n",
    "                serie_pt_objet.get_instances_ordered()\n",
    "                nifti_array = serie_pt_objet.get_numpy_array()\n",
    "                study_uid = serie_pt_objet.get_series_details()['study']['StudyInstanceUID']\n",
    "\n",
    "                                #generation nifti PT\n",
    "                filename_pt = study_uid+'_'+'nifti_'+'PT'+'.nii'\n",
    "                serie_pt_objet.export_nifti(os.path.join(nifti_directory, filename_pt))\n",
    "                print(\"EXPORT NIFTI PT\")\n",
    "                                #generation nifti CT\n",
    "                serie_ct_objet = SeriesCT(serie[2])\n",
    "                serie_ct_objet.get_instances_ordered()\n",
    "                serie_ct_objet.get_numpy_array()\n",
    "                filename_ct = study_uid+'_'+'nifti_'+'CT'+'.nii'\n",
    "                serie_ct_objet.export_nifti(os.path.join(nifti_directory, filename_ct))\n",
    "                print(\"EXPORT NIFTI CT\")\n",
    "\n",
    "        else : \n",
    "                serie_pt_objet = SeriesPT(serie[2])\n",
    "                serie_pt_objet.get_instances_ordered() \n",
    "                nifti_array = serie_pt_objet.get_numpy_array()\n",
    "                study_uid = serie_pt_objet.get_series_details()['study']['StudyInstanceUID']\n",
    "\n",
    "                        \n",
    "                                #generation nifti PT\n",
    "                filename_pt = study_uid+'_'+'nifti_'+'PT'+'.nii'\n",
    "                serie_pt_objet.export_nifti(os.path.join(nifti_directory, filename_pt))\n",
    "                print(\"EXPORT NIFTI PT\")\n",
    "                                #generation nifti CT\n",
    "                serie_ct_objet = SeriesCT(serie[0])\n",
    "                serie_ct_objet.get_instances_ordered()\n",
    "                serie_ct_objet.get_numpy_array()\n",
    "                filename_ct = study_uid+'_'+'nifti_'+'CT'+'.nii'\n",
    "                serie_ct_objet.export_nifti(os.path.join(nifti_directory, filename_ct))\n",
    "                print(\"EXPORT NIFTI CT\")\n",
    " \n",
    "\n",
    "    except Exception as err : \n",
    "        print(serie)\n",
    "        print(err)\n",
    "        error.append(serie)"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python38564bit9ddb669efcd540bea444ed93f0604dc5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
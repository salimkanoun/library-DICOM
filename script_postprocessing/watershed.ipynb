{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38264bit4afc185bc98b42c389abdbb3fbeec8dd",
   "display_name": "Python 3.8.2 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from library_dicom.dicom_processor.tools.create_mip import mip_imshow, mip_projection\n",
    "import matplotlib.pyplot as plt\n",
    "#from radiomics.featureextractor import *\n",
    "from sklearn import mixture\n",
    "import SimpleITK as sitk\n",
    "from PIL import Image\n",
    "\n",
    "from library_dicom.post_processing.PostProcess_Reader import PostProcess_Reader\n",
    "from library_dicom.post_processing.Mask4D import Mask4D\n",
    "from library_dicom.post_processing.WatershedModel import WatershedModel \n",
    "from library_dicom.dicom_processor.tools.folders import *\n",
    "from library_dicom.dicom_processor.tools.threshold_mask import *\n",
    "from library_dicom.dicom_processor.tools.postprocessing import *\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions \n",
    "\n",
    "pred_1 = os.listdir('/media/deeplearning/Elements/train')\n",
    "pred_2 = os.listdir('/media/deeplearning/Elements/val')\n",
    "\n",
    "pred = pred_1 + pred_2\n",
    "\n",
    "study_uid = []\n",
    "\n",
    "for item in pred_1 : \n",
    "    subliste = []\n",
    "    subliste.append(item.split('_')[0]) \n",
    "    subliste.append('train')\n",
    "    study_uid.append(subliste)\n",
    "\n",
    "for item in pred_2 : \n",
    "    subliste = []\n",
    "    subliste.append(item.split('_')[0]) \n",
    "    subliste.append('val')\n",
    "    study_uid.append(subliste)\n",
    "\n",
    "\n",
    "#charger csv agl, gained, flip \n",
    "csv_ahl=  '/media/deeplearning/Elements/AHL2011_NIFTI.csv'\n",
    "\n",
    "with open(csv_ahl, 'r') as csv_file :\n",
    "    reader = csv.reader(csv_file, delimiter = ',') #liste pour chaque ligne \n",
    "    data_ahl = []\n",
    "    for row in reader :\n",
    "        data_ahl.append(row)\n",
    "        \n",
    "del data_ahl[0] #enlever première ligne\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "csv_gained= '/media/deeplearning/78ca2911-9e9f-4f78-b80a-848024b95f92/GAINED_NIFTI/GAINED_PET0_NIFTI.csv'\n",
    "\n",
    "with open(csv_gained, 'r') as csv_file :\n",
    "    reader = csv.reader(csv_file, delimiter = ',') #liste pour chaque ligne \n",
    "    data_gained = []\n",
    "    for row in reader :\n",
    "        data_gained.append(row)\n",
    "        \n",
    "del data_gained[0] #enlever première ligne\n",
    "\n",
    "\n",
    "\n",
    "csv_flip= '/media/deeplearning/78ca2911-9e9f-4f78-b80a-848024b95f92/FLIP_NIFTI/FLIP_PET0_NIFTI.csv'\n",
    "\n",
    "with open(csv_flip, 'r') as csv_file :\n",
    "    reader = csv.reader(csv_file, delimiter = ',') #liste pour chaque ligne \n",
    "    data_flip = []\n",
    "    for row in reader :\n",
    "        data_flip.append(row)\n",
    "        \n",
    "del data_flip[0] #enlever première ligne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find associated serie nifti \n",
    "dataset = []\n",
    "for item in study_uid : \n",
    "    subliste = []\n",
    "    subliste.append(item[0])\n",
    "    subliste.append(item[1])\n",
    "    for ahl in data_ahl : \n",
    "        if item[0] in ahl : \n",
    "            subliste.append(ahl[1]) #patient id \n",
    "            subliste.append(ahl[5]) #pet\n",
    "            subliste.append(ahl[6]) #mask\n",
    "            subliste.append('AHL')\n",
    "            dataset.append(subliste)\n",
    "        \n",
    "    for gained in data_gained : \n",
    "        if item[0] in gained : \n",
    "            subliste.append(gained[0]) #patient id \n",
    "            subliste.append(gained[4]) #pet\n",
    "            subliste.append(gained[5]) #mask \n",
    "            subliste.append('GAINED')\n",
    "            dataset.append(subliste)\n",
    "\n",
    "    for flip in data_flip : \n",
    "        if item[0] in flip : \n",
    "            subliste.append(flip[1]) #patient id \n",
    "            subliste.append(flip[5]) #pet\n",
    "            subliste.append(flip[6]) #mask \n",
    "            subliste.append('FLIP')\n",
    "            dataset.append(subliste)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include prediction \n",
    "for serie in dataset : \n",
    "    for prediction in pred :\n",
    "        if serie[0] in prediction : \n",
    "            serie.append(prediction)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rewrite path for ahl and relative path for pred\n",
    "for serie in dataset : \n",
    "    if 'AHL' in serie : \n",
    "        txt = serie[3].replace('storage', 'deeplearning/Elements')\n",
    "        serie[3] = txt \n",
    "\n",
    "        txt_2 = serie[4].replace('storage', 'deeplearning/Elements')\n",
    "        serie[4] = txt_2 \n",
    "\n",
    "    if 'train' in serie : \n",
    "        path = '/media/deeplearning/Elements'+'/'+'train'+'/'+serie[-1]\n",
    "        serie[-1] = path \n",
    "\n",
    "    else : \n",
    "        path = '/media/deeplearning/Elements'+'/'+'val'+'/'+serie[-1]\n",
    "        serie[-1] = path \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write csv \n",
    "nifti_directory = '/media/deeplearning/78ca2911-9e9f-4f78-b80a-848024b95f92/post_processing/pred_2'\n",
    "\n",
    "with open(os.path.join(nifti_directory, 'dataset_post_processing_pred2.csv'), 'w') as csv_file : \n",
    "    csv_writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    csv_writer.writerow([\"PATIENT ID\", \"STUDY UID\", \"NIFTI_PET\", \"NIFTI_MASK\", \"PRED_MASK\", \"TYPE\", \"STUDY\"])\n",
    "    for serie in dataset:\n",
    "        csv_writer.writerow([serie[2], serie[0], serie[3], serie[4], serie[6], serie[1], serie[5]])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read csv \n",
    "csv_ahl_post_process= '/media/deeplearning/78ca2911-9e9f-4f78-b80a-848024b95f92/post_processing/pred_2/dataset_post_processing_pred2.csv'\n",
    "\n",
    "with open(csv_ahl_post_process, 'r') as csv_file :\n",
    "    reader = csv.reader(csv_file, delimiter = ',') #liste pour chaque ligne \n",
    "    dataset = []\n",
    "    for row in reader :\n",
    "        dataset.append(row)\n",
    "        \n",
    "del dataset[0] #enlever première ligne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "results_directory = '/media/deeplearning/78ca2911-9e9f-4f78-b80a-848024b95f92/post_processing/pred_2'\n",
    "error = []\n",
    "#path_mip = []\n",
    "\n",
    "\n",
    "for serie in dataset: \n",
    "    print(dataset.index(serie))\n",
    "    subliste = []\n",
    "    try : \n",
    "        model = WatershedModel(serie[4], serie[2], type = '3d')\n",
    "        ws_array, label_number = model.watershed_model(0.5)\n",
    "\n",
    "\n",
    "        volume_voxel = model.pet_spacing[0]*model.pet_spacing[1]*model.pet_spacing[2]*10**(-3) #ml \n",
    "\n",
    "\n",
    "        #VOLUME PREDICTION AVEC WATERSHED AFTER SEUIL 41%\n",
    "        ws_array = get_threshold_matrix(ws_array, model.pet_array, label_number, 0.41)\n",
    "        number_pixel = len(np.where(ws_array != 0)[0])\n",
    "        vol_sous_seg_seuil = number_pixel * volume_voxel\n",
    "        serie.append(vol_sous_seg_seuil)\n",
    "        #tmtv.append(vol_sous_seg_seuil)\n",
    "        print(\"VOL WITH WATERSHED OK\")\n",
    "\n",
    "\n",
    "        #VOLUME BINARY MASK \n",
    "        binary_img = model.get_binary_threshold_mask_img(0.5)\n",
    "        binary_array = model.remove_small_roi(binary_img)\n",
    "\n",
    "        number_pixel_binaire = len(np.where(binary_array != 0 )[0])\n",
    "        vol_binaire = number_pixel_binaire * volume_voxel \n",
    "        serie.append(vol_binaire)\n",
    "        print(\"VOL PRED OK \")\n",
    "\n",
    "        #VOLUME TRUTH- 41%\n",
    "        img_4d = sitk.ReadImage(serie[3])\n",
    "        mask = sitk.GetArrayFromImage(img_4d).transpose()\n",
    "        mask = get_threshold_matrix_4D(mask, model.pet_array, 0.41)\n",
    "\n",
    "        if len(mask.shape) != 3 : \n",
    "            mask_sum = np.sum(mask, axis = -1)\n",
    "        else : \n",
    "            mask_sum = mask \n",
    "\n",
    "        number_pixel_truth = len(np.where(mask_sum != 0)[0])\n",
    "        vol_truth = number_pixel_truth * volume_voxel\n",
    "        serie.append(vol_truth) \n",
    "        print(\"VOL TRUTH OK \")\n",
    "\n",
    "\n",
    "        #here to write stats results from the watershed labelled 3D matrix\n",
    "        #json_details = model.label_stat_results(img2)\n",
    "        #tmtv.append(json_details['total_vol'])\n",
    "        #write_json_file(folder, serie[1]+'_details', json_details)\n",
    "\n",
    "        #can add method to save result as nifti \n",
    "\n",
    "    except Exception as err : \n",
    "        print(err)\n",
    "        print(serie)\n",
    "        error.append(serie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write csv \n",
    "nifti_directory = '/media/deeplearning/78ca2911-9e9f-4f78-b80a-848024b95f92/post_processing/pred_2'\n",
    "\n",
    "with open(os.path.join(nifti_directory, 'result_tmtv_sous_seg.csv'), 'w') as csv_file : \n",
    "    csv_writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    csv_writer.writerow([\"PATIENT ID\", \"STUDY UID\", \"NIFTI_PET\", \"NIFTI_MASK\", \"PRED_MASK\", \"TYPE\", \"STUDY\", \"VOL_TRUTH\", \"VOL_PRED\", \"VOL_WS\"])\n",
    "    for serie in dataset:\n",
    "        csv_writer.writerow([serie[0], serie[1], serie[2], serie[3], serie[4], serie[5], serie[6], serie[-1], serie[-2], serie[-3]])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here, can add pourcent of difference between VOL_WS-41% and VOL_TRUTH , and plot the Bland Altmann result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open CSV with VOLUME result \n",
    "csv_ahl_post_process= '/media/deeplearning/78ca2911-9e9f-4f78-b80a-848024b95f92/post_processing/pred_2/result_tmtv_sous_seg.csv'\n",
    "\n",
    "with open(csv_ahl_post_process, 'r') as csv_file :\n",
    "    reader = csv.reader(csv_file, delimiter = ',') #liste pour chaque ligne \n",
    "    csv_data = []\n",
    "    for row in reader :\n",
    "        csv_data.append(row)\n",
    "        \n",
    "del csv_data[0] #enlever première ligne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calcul percentage of difference \n",
    "for row in csv_data : \n",
    "    vol_truth = float(row[-3])\n",
    "    vol_ws = float(row[-1])\n",
    "    diff = (vol_ws - vol_truth) / vol_truth * 100 \n",
    "\n",
    "    row.append(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get liste of all volume and percentage, only on vol_prediction bigger than volume_truth , compteur cpt to count it \n",
    "truth = []\n",
    "sous_seg = []\n",
    "pourcent = []\n",
    "pred = []\n",
    "cpt = 0\n",
    "for row in csv_data : \n",
    "    if float(row[-3]) > float(row[-4]) : #if vol pred > vol truth\n",
    "        cpt += 1 \n",
    "        truth.append(float(row[-4]))\n",
    "        sous_seg.append(float(row[-2]))\n",
    "        pourcent.append(float(row[-1]))\n",
    "        pred.append(float(row[-3]))\n",
    "\n",
    "print(cpt)\n",
    "print(len(truth))\n",
    "print(len(sous_seg))\n",
    "print(len(pourcent))\n",
    "print(len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bland Altmann plot \n",
    "mean = []\n",
    "diff = []\n",
    "for i in range(len(truth)):\n",
    "    subliste = []\n",
    "    subliste.append(truth[i])\n",
    "    subliste.append(sous_seg[i])\n",
    "    mean.append(np.mean(subliste))\n",
    "\n",
    "    diff.append(truth[i] - sous_seg[i])\n",
    "\n",
    "#y = pourcent \n",
    "f = plt.figure(figsize=(8,5))\n",
    "axes = plt.gca()\n",
    "moy = np.mean(pourcent)\n",
    "sd = np.std(pourcent)\n",
    "print(moy)\n",
    "print(sd)\n",
    "plt.axhline(moy, c='r', label='mean', linewidth=3)\n",
    "plt.axhline(moy + 2*sd, c='green', label = 'm + 2sd' )\n",
    "plt.axhline(moy - 2*sd, c='green', label = 'm - 2sd' )\n",
    "plt.axhline(0, c='black', linestyle='--',label = '0' )\n",
    "plt.axhline(20, c='blue',label = '20%', linestyle='dashdot' )\n",
    "plt.axhline(-20, c='blue',label = '-20%' ,linestyle='dashdot' )\n",
    "plt.scatter(mean, pourcent, alpha=0.6)\n",
    "plt.ylim(-200, 300)\n",
    "plt.xlabel('mean')\n",
    "plt.ylabel('pourcent diff')\n",
    "plt.legend()\n",
    "plt.title('Bland Altman after watershed')\n",
    "\n",
    "#y = difference\n",
    "f = plt.figure(figsize=(8,5))\n",
    "axes = plt.gca()\n",
    "moy = np.mean(diff)\n",
    "sd = np.std(diff)\n",
    "print(moy)\n",
    "print(sd)\n",
    "plt.axhline(moy, c='r', label='mean', linewidth=3)\n",
    "plt.axhline(moy + 2*sd, c='green', label = 'm + 2sd' )\n",
    "plt.axhline(moy - 2*sd, c='green', label = 'm - 2sd' )\n",
    "plt.scatter(mean, diff, alpha=0.6)\n",
    "#plt.xlim(0, 500)\n",
    "plt.xlabel('mean')\n",
    "plt.ylabel('diff')\n",
    "plt.legend()\n",
    "plt.title('Bland Altman after watershed')\n",
    "\n",
    "\n",
    "#linear regression \n",
    "from scipy import stats\n",
    "#linregress() renvoie plusieurs variables de retour. On s'interessera \n",
    "# particulierement au slope et intercept\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(truth, sous_seg)\n",
    "\n",
    "def regression(x, slope, intercept):\n",
    "    y = []\n",
    "    for i in range(len(x)):\n",
    "        y.append(slope * x[i] + intercept)\n",
    "\n",
    "    return y \n",
    "\n",
    "\n",
    "f = plt.figure(figsize=(8,5))\n",
    "axes = plt.gca()\n",
    "plt.scatter(truth, sous_seg, alpha=0.6)\n",
    "plt.plot(truth, regression(truth, slope, intercept), c='green', label='regression lin')\n",
    "plt.plot([0,2300], [0,2300], 'r--', label='x=y')\n",
    "plt.xlabel('truth')\n",
    "plt.ylabel('sous_seg')"
   ]
  }
 ]
}
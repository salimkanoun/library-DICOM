{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38264bit4afc185bc98b42c389abdbb3fbeec8dd",
   "display_name": "Python 3.8.2 64-bit",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from library_dicom.dicom_processor.tools.pyradiomic import *\n",
    "import SimpleITK as sitk \n",
    "import os \n",
    "import numpy as np \n",
    "import csv\n",
    "import json \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AHL NIFTI \n",
    "csv_path= '/media/deeplearning/78ca2911-9e9f-4f78-b80a-848024b95f92/AHL/AHL2011_dataset_dmax.csv'\n",
    "with open(csv_path, 'r') as csv_file :\n",
    "    reader = csv.reader(csv_file, delimiter = ',') #liste pour chaque ligne \n",
    "    csv_file_ = []\n",
    "    for row in reader :\n",
    "        csv_file_.append(row)\n",
    "        \n",
    "del csv_file_[0] #enlever première ligne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Nombre de mask à traiter : \", len(csv_file_))"
   ]
  },
  {
   "source": [
    "#### - Keep only pet 0 "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pet0 = []\n",
    "for row in csv_file_ : \n",
    "    if 'pet0' in row : \n",
    "        pet0.append(row)\n",
    "print(len(pet0))"
   ]
  },
  {
   "source": [
    "#### - Rewrite abs path "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/media/deeplearning/LACIE SHARE'\n",
    "for row in pet0 : \n",
    "    ct = row[-3]\n",
    "    pt = row[-2]\n",
    "    mask = row[-1]\n",
    "    up_ct = os.path.join(base_path, ct)\n",
    "    up_pt = os.path.join(base_path, pt)\n",
    "    up_mask = os.path.join(base_path, mask)\n",
    "    row[-3] = up_ct \n",
    "    row[-2] = up_pt \n",
    "    row[-1] = up_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get date \n",
    "csv_path_2= '/media/deeplearning/LACIE SHARE/AHL_NIFTI/pet0.csv'\n",
    "with open(csv_path_2, 'r') as csv_file :\n",
    "    reader = csv.reader(csv_file, delimiter = ';') #liste pour chaque ligne \n",
    "    csv_file_2 = []\n",
    "    for row in reader :\n",
    "        csv_file_2.append(row)\n",
    "        \n",
    "del csv_file_2[0] #enlever première ligne\n",
    "\n",
    "L = []\n",
    "for data in pet0 : \n",
    "    liste = []\n",
    "    pat_id = data[1]\n",
    "    for row in csv_file_2 : \n",
    "        if pat_id == row[1] : \n",
    "            liste.append(pat_id)\n",
    "            liste.append(row[4])\n",
    "            break\n",
    "    L.append(liste)\n",
    "\n",
    "m = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']\n",
    "c = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "for sub in L :\n",
    "    date = sub[1].split('_')\n",
    "    #print(date)\n",
    "    month = date[2].split(' ')[0]\n",
    "    day = date[2].split(' ')[1]\n",
    "    if len(date) == 5 :\n",
    "        year = date[3]\n",
    "    else : year = date[-1].split('-')[0].split('.')[0]\n",
    "    index = m.index(month)\n",
    "    new = []\n",
    "    new.append(year)\n",
    "    new.append(c[index])\n",
    "    new.append(day)\n",
    "    #print(new)\n",
    "    sub.append('_'.join(new))\n",
    "    \n",
    "for i in range(len(L)):\n",
    "    pet0[i].append(L[i][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "for nifti in pet0 : \n",
    "    try : \n",
    "        print(pet0.index(nifti))\n",
    "        mask_path = nifti[-2]\n",
    "        pet_path = nifti[-3]\n",
    "        liste_center = get_center_of_mass(mask_path, thresh = True, pet_path = pet_path)\n",
    "        d_max = calcul_distance_max(liste_center)\n",
    "        nifti.append(d_max)\n",
    "    except Exception as err : \n",
    "        print(nifti)\n",
    "        print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nifti_directory = '/media/deeplearning/LACIE SHARE/AHL_NIFTI'\n",
    "filename = 'AHL2011_dMax_v4.csv'\n",
    "\n",
    "with open(os.path.join(nifti_directory, filename), 'w') as csv_file : \n",
    "    csv_writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    csv_writer.writerow([\"PATIENT ID\", \"DATE\", \"STUDY_UID\", \"D_MAX\"])\n",
    "    for row in pet0: \n",
    "        csv_writer.writerow([row[1], row[-2], row[2], row[-1]])\n",
    "    for row in dataset : \n",
    "        csv_writer.writerow([row[0], row[1], row[2], row[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bit9a8409779cd04c3cbd0c5f1e859a644e",
   "display_name": "Python 3.8.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from radiomics.featureextractor import RadiomicsFeatureExtractor \n",
    "import SimpleITK as sitk \n",
    "import os \n",
    "import numpy as np \n",
    "from library_dicom.dicom_processor.tools.pyradiomic import *\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path= '/media/deeplearning/Elements/AHL2011_NIFTI.csv'\n",
    "csv_file = []\n",
    "with open(csv_path, 'r') as csv_file :\n",
    "    reader = csv.reader(csv_file, delimiter = ',') #liste pour chaque ligne \n",
    "    csv_file = []\n",
    "    for row in reader :\n",
    "        csv_file.append(row)\n",
    "        \n",
    "del csv_file[0] #enlever première ligne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for row in csv_file : \n",
    "    if 'pet0' in row : \n",
    "        dataset.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Nombre de mask à traiter :\", len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#rename path \n",
    "\n",
    "dataset[0]\n",
    "for data in dataset : \n",
    "    mask = \"/\".join(data[-1].split('/')[-2:])\n",
    "    path = os.path.join('/media/deeplearning/Elements' ,mask)\n",
    "\n",
    "    pet = \"/\".join(data[-2].split('/')[-2:])\n",
    "    pet_path = os.path.join('/media/deeplearning/Elements' ,pet)\n",
    "    data[-1] = path\n",
    "    data[-2] = pet_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find date in json \n",
    "import json \n",
    "liste_json = []\n",
    "list_dir = os.listdir('/media/deeplearning/Elements/AHL_JSON')\n",
    "for file_ in list_dir : \n",
    "    liste_json.append(os.path.join('/media/deeplearning/Elements/AHL_JSON', file_))\n",
    "\n",
    "\n",
    "for data in dataset : \n",
    "    for json_path in liste_json : \n",
    "        with open(json_path) as json_file : \n",
    "            reader = json.load(json_file)\n",
    "\n",
    "            uid = reader['study']['StudyInstanceUID']\n",
    "\n",
    "            if uid == data[2] : \n",
    "\n",
    "                data.append(reader['study']['StudyDate'])\n",
    "\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in dataset : \n",
    "    year = data[-1][0:4]\n",
    "    month = data[-1][4:6]\n",
    "    day = data[-1][6:8]\n",
    "    l = [year, month, day]\n",
    "    date = \"_\".join(l)\n",
    "\n",
    "    data.append(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if every data has its date\n",
    "cpt = 0\n",
    "for data in dataset : \n",
    "    if len(data) != 9 : \n",
    "        cpt += 1\n",
    "\n",
    "cpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "distance_max = []\n",
    "for nifti in dataset: \n",
    "    try : \n",
    "        print(dataset.index(nifti))\n",
    "        liste_center = get_center_of_mass(nifti[-3], thresh = True, pet_path=nifti[-4])\n",
    "        d_max = calcul_distance_max(liste_center)\n",
    "        nifti.append(d_max)\n",
    "    except Exception as err : \n",
    "        print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good = []\n",
    "problem = []\n",
    "for data in dataset : \n",
    "    if len(data) == 10 : \n",
    "        good.append(data)\n",
    "    else : \n",
    "        problem.append(data)"
   ]
  },
  {
   "source": [
    "print(len(problem), \"problem during extracting features\")"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nifti_directory = '/home/deeplearning/AHL'\n",
    "filename = 'AHL2011_dMax_v3.csv'\n",
    "\n",
    "with open(os.path.join(nifti_directory, filename), 'w') as csv_file : \n",
    "    csv_writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    csv_writer.writerow([\"PATIENT ID\", \"DATE\", \"STUDY_UID\", \"D_MAX\"])\n",
    "    for row in good: \n",
    "        csv_writer.writerow([row[1], row[-2], row[2], row[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
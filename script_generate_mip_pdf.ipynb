{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bit9a8409779cd04c3cbd0c5f1e859a644e",
   "display_name": "Python 3.8.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from library_dicom.dicom_processor.tools.create_mip import *\n",
    "from library_dicom.dicom_processor.tools.postprocessing import *\n",
    "from library_dicom.dicom_processor.tools.folders import *\n",
    "import os \n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/media/deeplearning/Elements/inference/RELEVANCE_NIFTI'\n",
    "list_dir = os.listdir(folder)\n",
    "liste_inference = []\n",
    "for file_ in list_dir : \n",
    "    sub = []\n",
    "    sub.append(os.path.join(folder, file_))\n",
    "    #liste_inference.append(os.path.join(folder, file_))\n",
    "    sub.append(file_.split('_')[0])\n",
    "    #liste_study_uid.append(file_.split('_')[0])\n",
    "    liste_inference.append(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path= '/media/deeplearning/Elements/RELEVANCE_NIFTI/RELEVANCE_NIFTI_SCREENING_v3.csv'\n",
    "\n",
    "with open(csv_path, 'r') as csv_file :\n",
    "    reader = csv.reader(csv_file, delimiter = ',') #liste pour chaque ligne \n",
    "    dataset = []\n",
    "    for row in reader :\n",
    "        dataset.append(row)\n",
    "        \n",
    "del dataset[0] #enlever première ligne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#with study uid, find path pet \n",
    "for study in liste_inference : \n",
    "    for row in dataset : \n",
    "        if study[1] == row[1] : \n",
    "            study.append(row[-1]) #pet path \n",
    "            study.append(row[0]) #patient id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_ = []\n",
    "for study in liste_inference : \n",
    "    if len(study) != 4 : \n",
    "        remove_.append(study)\n",
    "\n",
    "for r in remove_: \n",
    "    liste_inference.remove(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Nombre de prédictions : ', len(liste_inference))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mip_directory = '/media/deeplearning/Elements/RELEVANCE_MIP'\n",
    "filenames = []\n",
    "cpt =0\n",
    "for study in liste_inference : \n",
    "   print(cpt)\n",
    "   uid = study[1]\n",
    "   sub = []\n",
    "   inference_array = read_inference(study[0])\n",
    "   pet_array, size = read_img(study[2])\n",
    "\n",
    "   sub.append(mip_projection(pet_array, 90, mip_directory, uid, 'pet', cmap = 'gray', vmin= 0 , vmax = 6))\n",
    "   sub.append(mip_superposition_show(inference_array, pet_array, 90, uid, vmin = 0, vmax = 6, cmap_pet = 'gray', cmap_mask = 'hsv', alpha = 0.6, save = True, directory = mip_directory))\n",
    "   sub.append(uid)\n",
    "   filenames.append(sub)\n",
    "   cpt += 1"
   ]
  },
  {
   "source": [
    "err = '1.2.840.113704.1.111.4212.1406734680.46'\n",
    "remove_ = []\n",
    "for mip in filenames : \n",
    "    if err in mip : \n",
    "        remove_.append(mip)\n",
    "\n",
    "for r in remove_ : \n",
    "    filenames.remove(r)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_json_file(mip_directory, 'filenames', filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path_name = mip_directory+'/'+'mip'\n",
    "create_pdf_mip(filenames, output_path_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXCLUDED MASK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_uid = ['1.2.840.113619.2.55.3.34213890.661.1394086511.913',\n",
    "'1.3.46.670589.16.2.2.164.1.157.1.20140415.141816.4561358',\n",
    "'1.3.51.0.1.1.10.49.10.222.2458375.2457056',\n",
    "'2.16.56.465769650.2551326252.2240783396',\n",
    "'2.16.56.465769650.2551326252.2606294451',\n",
    "'1.2.276.0.7230010.3.8699790']\n",
    "\n",
    "study = []\n",
    "for uid in liste_uid : \n",
    "    for row in dataset : \n",
    "        if uid == row[1] : \n",
    "            study.append(row)\n",
    "\n",
    "predictions = []\n",
    "for uid in liste_uid : \n",
    "    for data in liste_inference : \n",
    "        if uid == data[1] : \n",
    "            predictions.append(data)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nifti_directory = '/media/deeplearning/Elements/inference'\n",
    "filename = 'excluded_mask_pred.csv'\n",
    "\n",
    "\n",
    "with open(os.path.join(nifti_directory, filename), 'w') as csv_file : \n",
    "    csv_writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    csv_writer.writerow([\"PATIENT ID\", \"STUDY UID\", \"INFERENCE\"])\n",
    "    for s, pred in zip(study, predictions) : \n",
    "        csv_writer.writerow([s[0], s[1], pred[0]])"
   ]
  }
 ]
}
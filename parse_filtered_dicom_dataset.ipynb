{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "from library_dicom.dicom_processor.tools.folders import *\n",
    "from library_dicom.dicom_processor.tools.series import get_series_object\n",
    "\n",
    "import csv\n",
    "from library_dicom.dicom_processor.model.Series import Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_paths = get_series_path('/home/deeplearning/AHL/AHL_Validated_DICOM')\n",
    "export_folder = '/home/deeplearning/AHL/AHL_JSON'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of serie \n",
    "len(series_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "index_problem = []\n",
    "for serie_path in series_paths:\n",
    "    print(series_paths.index(serie_path))\n",
    "    try:\n",
    "        dicom_serie = get_series_object(serie_path)\n",
    "        dicomsInfo = dicom_serie.get_series_details()\n",
    "        write_json_file(export_folder, dicomsInfo['series']['SeriesInstanceUID'], dicomsInfo)\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "        print(dicomsInfo)\n",
    "        index_problem.append(series_paths.index(serie_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = '/home/deeplearning/AHL/AHL_JSON'\n",
    "list_dir_json = os.listdir(json_path)\n",
    "list_json = []\n",
    "for file_ in list_dir_json : \n",
    "    list_json.append([os.path.join(json_path, file_)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for liste in list_json : \n",
    "    print(list_json.index(liste))\n",
    "    with open(liste[0]) as json_file : \n",
    "        reader = json.load(json_file)\n",
    "\n",
    "        patient_id = reader[\"patient\"][\"PatientID\"]\n",
    "        study_uid = reader[\"study\"]['StudyInstanceUID']\n",
    "        modal = reader['series']['Modality']\n",
    "\n",
    "        path = reader['path']\n",
    "\n",
    "        type_ = reader['study'][\"AccessionNumber\"]\n",
    "\n",
    "        liste.append(path)\n",
    "        liste.append(modal)\n",
    "        liste.append(type_)\n",
    "        liste.append(patient_id)\n",
    "        liste.append(study_uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get every study_uid \n",
    "stud = []\n",
    "for liste in list_json : \n",
    "    stud.append(liste[-1])\n",
    "\n",
    "study_uid = []\n",
    "study_uid.append(stud[0])\n",
    "for uid in stud : \n",
    "    if uid not in study_uid : \n",
    "        study_uid.append(uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Nombre de study :', len(study_uid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for uid in study_uid : \n",
    "    sub = []\n",
    "    for liste in list_json : \n",
    "        if uid in liste : \n",
    "            sub.append(liste)\n",
    "    data.append(sub)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for serie in data : \n",
    "    serie[0].append(serie[1][0])\n",
    "    serie[0].append(serie[1][1])\n",
    "    serie[0].append(serie[1][2])\n",
    "\n",
    "    dataset.append(serie[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean dataset : if no json, if double study uid, if not CT&PT \n",
    "\n",
    "#no json \n",
    "\n",
    "for data in dataset : \n",
    "    if len(data) != 9 : \n",
    "        print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#double study_uid \n",
    "double = []\n",
    "double.append(dataset[0])\n",
    "for data in dataset : \n",
    "    if data not in double : \n",
    "        double.append(data)\n",
    "\n",
    "if len(double) == len(dataset) : \n",
    "    print('No double study uid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check Modality \n",
    "index = []\n",
    "for data in dataset : \n",
    "    modal = []\n",
    "    modal.append(data[2])\n",
    "    modal.append(data[-1])\n",
    "    print(modal)\n",
    "    if modal[0] == modal[1] : \n",
    "        index.append(dataset.index(data))\n",
    "print(index)\n",
    "#if index is not empty : \n",
    "if len(index) != 0 :\n",
    "    liste_to_remove = []\n",
    "    for ind in index : \n",
    "        liste_to_remove.append(dataset[ind])\n",
    "\n",
    "    for r in liste_to_remove : \n",
    "        dataset.remove(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, can prepare json file to generate nifti in \"nifti_builder_from_json.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "study_results = []\n",
    "for liste in dataset : \n",
    "    subliste = []\n",
    "    subliste.append(liste[1]) #path_1\n",
    "    subliste.append(liste[2]) #modal 1\n",
    "    subliste.append(liste[-2]) #path_2\n",
    "    subliste.append(liste[-1]) #modal_2\n",
    "    subliste.append(liste[5]) #study_uid\n",
    "    subliste.append(liste[3]) #type\n",
    "    subliste.append(liste[4]) #patient id\n",
    "\n",
    "    study_results.append(subliste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save results as json \n",
    "directory = '/home/deeplearning/AHL'\n",
    "filename = 'AHL_new_dataset'\n",
    "write_json_file(directory, filename, study_results)"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bit4afc185bc98b42c389abdbb3fbeec8dd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}